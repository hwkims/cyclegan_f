{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOhplJ5Uvn9bkIiJqIHu5rz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlkXlPm4q8gL","executionInfo":{"status":"ok","timestamp":1733904714277,"user_tz":-540,"elapsed":590652,"user":{"displayName":"현우","userId":"13721909872316796663"}},"outputId":"32eea954-40ab-40e9-b026-212f888be378"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], g_loss=0.8529413938522339, f_loss=0.6886417865753174, d_x_loss=0.6840558052062988, d_y_loss=0.676378607749939\n","Epoch [2/100], g_loss=0.7795727849006653, f_loss=0.722180962562561, d_x_loss=0.6899711489677429, d_y_loss=0.651207685470581\n","Epoch [3/100], g_loss=0.7673603296279907, f_loss=0.6862666606903076, d_x_loss=0.6972934603691101, d_y_loss=0.6616925001144409\n","Epoch [4/100], g_loss=0.7572587728500366, f_loss=0.7042737007141113, d_x_loss=0.6903238296508789, d_y_loss=0.6684834957122803\n","Epoch [5/100], g_loss=0.9822690486907959, f_loss=0.7348226308822632, d_x_loss=0.7007501125335693, d_y_loss=0.586349606513977\n","Epoch [6/100], g_loss=0.809486985206604, f_loss=0.6969208717346191, d_x_loss=0.6931134462356567, d_y_loss=0.7215025424957275\n","Epoch [7/100], g_loss=1.1146695613861084, f_loss=0.7617266774177551, d_x_loss=0.694053053855896, d_y_loss=0.518182635307312\n","Epoch [8/100], g_loss=0.6992472410202026, f_loss=0.6936072111129761, d_x_loss=0.6932144165039062, d_y_loss=0.5961412191390991\n","Epoch [9/100], g_loss=0.9539299011230469, f_loss=0.6873373985290527, d_x_loss=0.6972997188568115, d_y_loss=0.5960478186607361\n","Epoch [10/100], g_loss=1.3181016445159912, f_loss=0.6974103450775146, d_x_loss=0.6920045614242554, d_y_loss=0.6495068073272705\n","Epoch [11/100], g_loss=0.8350241184234619, f_loss=0.7081207036972046, d_x_loss=0.6929597854614258, d_y_loss=0.5366671085357666\n","Epoch [12/100], g_loss=1.0629894733428955, f_loss=0.7219805717468262, d_x_loss=0.6941869258880615, d_y_loss=0.5837196707725525\n","Epoch [13/100], g_loss=0.9648892879486084, f_loss=0.6731369495391846, d_x_loss=0.693071722984314, d_y_loss=0.6475856304168701\n","Epoch [14/100], g_loss=1.0962426662445068, f_loss=0.6901403665542603, d_x_loss=0.6917940974235535, d_y_loss=0.5164666175842285\n","Epoch [15/100], g_loss=0.9093050360679626, f_loss=0.7114953994750977, d_x_loss=0.6768035292625427, d_y_loss=0.6101058125495911\n","Epoch [16/100], g_loss=1.11350679397583, f_loss=0.676804780960083, d_x_loss=0.6891093254089355, d_y_loss=0.6361953020095825\n","Epoch [17/100], g_loss=0.9987504482269287, f_loss=0.7166532278060913, d_x_loss=0.6861122846603394, d_y_loss=0.5552411675453186\n","Epoch [18/100], g_loss=0.9768245816230774, f_loss=0.7054561376571655, d_x_loss=0.6949734687805176, d_y_loss=0.5432263612747192\n","Epoch [19/100], g_loss=1.031590461730957, f_loss=0.6969223022460938, d_x_loss=0.692482054233551, d_y_loss=0.5619227886199951\n","Epoch [20/100], g_loss=1.2746367454528809, f_loss=0.6869388818740845, d_x_loss=0.6898868083953857, d_y_loss=0.4920526146888733\n","Epoch [21/100], g_loss=1.1036629676818848, f_loss=0.6851009130477905, d_x_loss=0.6940696239471436, d_y_loss=0.5018830299377441\n","Epoch [22/100], g_loss=1.1261796951293945, f_loss=0.680275559425354, d_x_loss=0.6804764866828918, d_y_loss=0.5534716844558716\n","Epoch [23/100], g_loss=1.0399386882781982, f_loss=0.6746798157691956, d_x_loss=0.6840890645980835, d_y_loss=0.5592904090881348\n","Epoch [24/100], g_loss=1.1691203117370605, f_loss=0.6801361441612244, d_x_loss=0.6906347274780273, d_y_loss=0.5783950090408325\n","Epoch [25/100], g_loss=1.6255834102630615, f_loss=0.6821136474609375, d_x_loss=0.6922721862792969, d_y_loss=0.399777352809906\n","Epoch [26/100], g_loss=0.930637776851654, f_loss=0.6812947392463684, d_x_loss=0.702518105506897, d_y_loss=0.5967923998832703\n","Epoch [27/100], g_loss=0.9627178907394409, f_loss=0.690555214881897, d_x_loss=0.6896089315414429, d_y_loss=0.5848352909088135\n","Epoch [28/100], g_loss=0.7467291951179504, f_loss=0.6804315447807312, d_x_loss=0.6928964853286743, d_y_loss=0.6496533155441284\n","Epoch [29/100], g_loss=1.0283198356628418, f_loss=0.7777982950210571, d_x_loss=0.6743930578231812, d_y_loss=0.5429090261459351\n","Epoch [30/100], g_loss=0.8992643356323242, f_loss=0.6969366073608398, d_x_loss=0.6948403120040894, d_y_loss=0.5937509536743164\n","Epoch [31/100], g_loss=1.1929435729980469, f_loss=0.6900241374969482, d_x_loss=0.6915093660354614, d_y_loss=0.3884979486465454\n","Epoch [32/100], g_loss=1.002615213394165, f_loss=0.6918187141418457, d_x_loss=0.6953846216201782, d_y_loss=0.48987895250320435\n","Epoch [33/100], g_loss=1.1240205764770508, f_loss=0.7021059989929199, d_x_loss=0.6953958868980408, d_y_loss=0.49184370040893555\n","Epoch [34/100], g_loss=1.0577855110168457, f_loss=0.7803901433944702, d_x_loss=0.5989513993263245, d_y_loss=0.4451799988746643\n","Epoch [35/100], g_loss=0.949954628944397, f_loss=0.6555659770965576, d_x_loss=0.6770755052566528, d_y_loss=0.5018966197967529\n","Epoch [36/100], g_loss=0.9046155214309692, f_loss=0.6004201173782349, d_x_loss=0.7242740392684937, d_y_loss=0.6117734909057617\n","Epoch [37/100], g_loss=0.8424212336540222, f_loss=0.6674306392669678, d_x_loss=0.6831861734390259, d_y_loss=0.4818735718727112\n","Epoch [38/100], g_loss=0.8632131814956665, f_loss=0.6947984099388123, d_x_loss=0.6957321166992188, d_y_loss=0.697614312171936\n","Epoch [39/100], g_loss=1.209096074104309, f_loss=0.704885721206665, d_x_loss=0.6920211315155029, d_y_loss=0.5494484901428223\n","Epoch [40/100], g_loss=0.9968932867050171, f_loss=0.696985125541687, d_x_loss=0.6936699748039246, d_y_loss=0.6592382788658142\n","Epoch [41/100], g_loss=1.028275489807129, f_loss=0.6842054128646851, d_x_loss=0.6877237558364868, d_y_loss=0.4953053593635559\n","Epoch [42/100], g_loss=1.078231930732727, f_loss=0.683902382850647, d_x_loss=0.6950355768203735, d_y_loss=0.5308631062507629\n","Epoch [43/100], g_loss=1.1561470031738281, f_loss=0.6989469528198242, d_x_loss=0.6979226469993591, d_y_loss=0.5385777950286865\n","Epoch [44/100], g_loss=0.909388542175293, f_loss=0.7201966643333435, d_x_loss=0.6893820762634277, d_y_loss=0.5300973653793335\n","Epoch [45/100], g_loss=1.111246109008789, f_loss=0.6928184032440186, d_x_loss=0.693466067314148, d_y_loss=0.5277227759361267\n","Epoch [46/100], g_loss=1.0372095108032227, f_loss=0.7386907339096069, d_x_loss=0.6806598901748657, d_y_loss=0.6277188658714294\n","Epoch [47/100], g_loss=1.1643502712249756, f_loss=0.6979174613952637, d_x_loss=0.6920716762542725, d_y_loss=0.5780665874481201\n","Epoch [48/100], g_loss=1.0891399383544922, f_loss=0.6877343654632568, d_x_loss=0.6974660158157349, d_y_loss=0.5578240156173706\n","Epoch [49/100], g_loss=0.8260172009468079, f_loss=0.6239618062973022, d_x_loss=0.695655107498169, d_y_loss=0.5816695690155029\n","Epoch [50/100], g_loss=1.0177792310714722, f_loss=0.6991516351699829, d_x_loss=0.6813803911209106, d_y_loss=0.6123016476631165\n","Epoch [51/100], g_loss=0.9749877452850342, f_loss=0.733674168586731, d_x_loss=0.6895710229873657, d_y_loss=0.559613049030304\n","Epoch [52/100], g_loss=0.9322785139083862, f_loss=0.6895145773887634, d_x_loss=0.6900668144226074, d_y_loss=0.5936374664306641\n","Epoch [53/100], g_loss=0.9459638595581055, f_loss=0.6616063714027405, d_x_loss=0.6938436627388, d_y_loss=0.6306723356246948\n","Epoch [54/100], g_loss=1.128300666809082, f_loss=0.7126668691635132, d_x_loss=0.6908703446388245, d_y_loss=0.539926290512085\n","Epoch [55/100], g_loss=1.013796091079712, f_loss=0.6871260404586792, d_x_loss=0.6951299905776978, d_y_loss=0.5741775035858154\n","Epoch [56/100], g_loss=1.0801432132720947, f_loss=0.7861835956573486, d_x_loss=0.6800995469093323, d_y_loss=0.6406668424606323\n","Epoch [57/100], g_loss=1.1431913375854492, f_loss=0.7149108052253723, d_x_loss=0.6920561194419861, d_y_loss=0.5579916834831238\n","Epoch [58/100], g_loss=1.08302640914917, f_loss=0.7045249342918396, d_x_loss=0.689511775970459, d_y_loss=0.5919111967086792\n","Epoch [59/100], g_loss=1.2559524774551392, f_loss=0.7100843787193298, d_x_loss=0.6892726421356201, d_y_loss=0.4297600984573364\n","Epoch [60/100], g_loss=1.370570421218872, f_loss=0.704159140586853, d_x_loss=0.6923950910568237, d_y_loss=0.4437355399131775\n","Epoch [61/100], g_loss=1.0139024257659912, f_loss=0.5948120951652527, d_x_loss=0.6896872520446777, d_y_loss=0.5391336679458618\n","Epoch [62/100], g_loss=1.0695252418518066, f_loss=0.7255653142929077, d_x_loss=0.6893460750579834, d_y_loss=0.5220685005187988\n","Epoch [63/100], g_loss=1.0508980751037598, f_loss=0.7075740098953247, d_x_loss=0.6940227150917053, d_y_loss=0.5880547165870667\n","Epoch [64/100], g_loss=0.6783104538917542, f_loss=0.7028727531433105, d_x_loss=0.6910254955291748, d_y_loss=0.7361906170845032\n","Epoch [65/100], g_loss=1.320861577987671, f_loss=0.7043719291687012, d_x_loss=0.6946137547492981, d_y_loss=0.42123380303382874\n","Epoch [66/100], g_loss=1.0776212215423584, f_loss=0.7289121150970459, d_x_loss=0.6942610144615173, d_y_loss=0.5299527645111084\n","Epoch [67/100], g_loss=1.0511549711227417, f_loss=0.671859860420227, d_x_loss=0.6943104267120361, d_y_loss=0.5691159963607788\n","Epoch [68/100], g_loss=1.0721980333328247, f_loss=0.686601459980011, d_x_loss=0.6752119064331055, d_y_loss=0.5583357810974121\n","Epoch [69/100], g_loss=0.9977219700813293, f_loss=0.7054046392440796, d_x_loss=0.6932120323181152, d_y_loss=0.5181686282157898\n","Epoch [70/100], g_loss=1.0917993783950806, f_loss=0.7033741474151611, d_x_loss=0.7063225507736206, d_y_loss=0.5497077703475952\n","Epoch [71/100], g_loss=0.8310728669166565, f_loss=0.7856568694114685, d_x_loss=0.6738384366035461, d_y_loss=0.7589586973190308\n","Epoch [72/100], g_loss=1.0855395793914795, f_loss=0.6980568170547485, d_x_loss=0.6983163356781006, d_y_loss=0.6134285926818848\n","Epoch [73/100], g_loss=1.0684272050857544, f_loss=0.7302243113517761, d_x_loss=0.665799081325531, d_y_loss=0.5595735311508179\n","Epoch [74/100], g_loss=1.1107873916625977, f_loss=0.6901223659515381, d_x_loss=0.6924054622650146, d_y_loss=0.48403871059417725\n","Epoch [75/100], g_loss=1.1963680982589722, f_loss=0.7220418453216553, d_x_loss=0.6953858137130737, d_y_loss=0.5621597766876221\n","Epoch [76/100], g_loss=1.1942412853240967, f_loss=0.7224369049072266, d_x_loss=0.6944783329963684, d_y_loss=0.4527048170566559\n","Epoch [77/100], g_loss=1.4620862007141113, f_loss=0.7043107748031616, d_x_loss=0.6966553926467896, d_y_loss=0.4285486936569214\n","Epoch [78/100], g_loss=1.3110675811767578, f_loss=0.7008792757987976, d_x_loss=0.6916390657424927, d_y_loss=0.5158829689025879\n","Epoch [79/100], g_loss=1.3146121501922607, f_loss=0.6991539001464844, d_x_loss=0.6901811361312866, d_y_loss=0.5120737552642822\n","Epoch [80/100], g_loss=1.2705748081207275, f_loss=0.7016677260398865, d_x_loss=0.6953948736190796, d_y_loss=0.42835140228271484\n","Epoch [81/100], g_loss=1.226294755935669, f_loss=0.6522866487503052, d_x_loss=0.6968761682510376, d_y_loss=0.5128613710403442\n","Epoch [82/100], g_loss=1.66986882686615, f_loss=0.7276409864425659, d_x_loss=0.6693775653839111, d_y_loss=0.37122753262519836\n","Epoch [83/100], g_loss=1.0837669372558594, f_loss=0.7251183986663818, d_x_loss=0.6664265394210815, d_y_loss=0.5953900814056396\n","Epoch [84/100], g_loss=1.2452763319015503, f_loss=0.7246249914169312, d_x_loss=0.6922311782836914, d_y_loss=0.5238858461380005\n","Epoch [85/100], g_loss=1.206660270690918, f_loss=0.7147929072380066, d_x_loss=0.694157600402832, d_y_loss=0.6353480815887451\n","Epoch [86/100], g_loss=0.9831336736679077, f_loss=0.6863988637924194, d_x_loss=0.6916982531547546, d_y_loss=0.6103008985519409\n","Epoch [87/100], g_loss=1.536298394203186, f_loss=0.6562850475311279, d_x_loss=0.6808434724807739, d_y_loss=0.444429874420166\n","Epoch [88/100], g_loss=1.2759392261505127, f_loss=0.721190869808197, d_x_loss=0.6787164211273193, d_y_loss=0.527172863483429\n","Epoch [89/100], g_loss=1.2508361339569092, f_loss=0.7317843437194824, d_x_loss=0.6900595426559448, d_y_loss=0.49927958846092224\n","Epoch [90/100], g_loss=1.314723014831543, f_loss=0.7196445465087891, d_x_loss=0.6425315141677856, d_y_loss=0.5847102403640747\n","Epoch [91/100], g_loss=1.2263892889022827, f_loss=0.6796844005584717, d_x_loss=0.6926881074905396, d_y_loss=0.464144766330719\n","Epoch [92/100], g_loss=1.4036332368850708, f_loss=0.7570008039474487, d_x_loss=0.7031680345535278, d_y_loss=0.35428065061569214\n","Epoch [93/100], g_loss=1.4090120792388916, f_loss=0.7352827787399292, d_x_loss=0.6690372228622437, d_y_loss=0.5126998424530029\n","Epoch [94/100], g_loss=1.2222917079925537, f_loss=0.6875630617141724, d_x_loss=0.6909726858139038, d_y_loss=0.5723404884338379\n","Epoch [95/100], g_loss=1.1258745193481445, f_loss=0.6827269792556763, d_x_loss=0.7058707475662231, d_y_loss=0.5021145343780518\n","Epoch [96/100], g_loss=1.048492431640625, f_loss=0.7507784366607666, d_x_loss=0.7024077773094177, d_y_loss=0.5553821325302124\n","Epoch [97/100], g_loss=1.3229565620422363, f_loss=0.6522851586341858, d_x_loss=0.661540150642395, d_y_loss=0.5934632420539856\n","Epoch [98/100], g_loss=1.1348401308059692, f_loss=0.7687475681304932, d_x_loss=0.6655595898628235, d_y_loss=0.5509490966796875\n","Epoch [99/100], g_loss=1.2572768926620483, f_loss=0.7936954498291016, d_x_loss=0.6701953411102295, d_y_loss=0.4904704689979553\n","Epoch [100/100], g_loss=1.1641521453857422, f_loss=0.7547386288642883, d_x_loss=0.6569619178771973, d_y_loss=0.4785171449184418\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# 데이터셋 로딩 및 변환\n","class ImageDataset(Dataset):\n","    def __init__(self, photo_dir, sketch_dir, transform=None):\n","        self.photo_paths = sorted([os.path.join(photo_dir, x) for x in os.listdir(photo_dir)])\n","        self.sketch_paths = sorted([os.path.join(sketch_dir, x) for x in os.listdir(sketch_dir)])\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.photo_paths)\n","\n","    def __getitem__(self, idx):\n","        photo = Image.open(self.photo_paths[idx]).convert('RGB')\n","        sketch = Image.open(self.sketch_paths[idx]).convert('RGB')\n","\n","        if self.transform:\n","            photo = self.transform(photo)\n","            sketch = self.transform(sketch)\n","\n","        return photo, sketch\n","\n","# 이미지 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),\n","    transforms.RandomCrop(256),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# 데이터셋 준비\n","dataset = ImageDataset('/content/drive/MyDrive/GAN_tutorial-main/photos', '/content/drive/MyDrive/GAN_tutorial-main/sketches', transform)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Generator 정의 (UNet 스타일)\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 정의\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# 생성자와 판별자 모델 초기화\n","generator_g = Generator().cuda()  # 사진 -> 스케치\n","generator_f = Generator().cuda()  # 스케치 -> 사진\n","discriminator_x = Discriminator().cuda()  # 판별자 (스케치)\n","discriminator_y = Discriminator().cuda()  # 판별자 (사진)\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = nn.BCELoss()\n","lambda_cycle = 10.0  # Cycle consistency loss scale\n","optimizer_g = optim.Adam(list(generator_g.parameters()) + list(generator_f.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_d = optim.Adam(list(discriminator_x.parameters()) + list(discriminator_y.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 훈련 함수\n","def train_step(real_x, real_y):\n","    batch_size = real_x.size(0)\n","    valid = torch.ones(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","    fake = torch.zeros(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","\n","    # Generator 학습\n","    optimizer_g.zero_grad()\n","\n","    # 생성자 g의 손실 (스케치 -> 사진)\n","    fake_x = generator_g(real_y)\n","    g_loss = criterion(discriminator_y(fake_x), valid)  # 스케치 -> 사진 판별자\n","    g_loss.backward()\n","\n","    # 생성자 f의 손실 (사진 -> 스케치)\n","    fake_y = generator_f(real_x)\n","    f_loss = criterion(discriminator_x(fake_y), valid)  # 사진 -> 스케치 판별자\n","    f_loss.backward()\n","\n","    optimizer_g.step()\n","\n","    # Discriminator 학습\n","    optimizer_d.zero_grad()\n","\n","    # 판별자 x (스케치 판별)\n","    d_x_real_loss = criterion(discriminator_x(real_y), valid)\n","    d_x_fake_loss = criterion(discriminator_x(fake_y.detach()), fake)\n","    d_x_loss = (d_x_real_loss + d_x_fake_loss) * 0.5\n","\n","    # 판별자 y (사진 판별)\n","    d_y_real_loss = criterion(discriminator_y(real_x), valid)\n","    d_y_fake_loss = criterion(discriminator_y(fake_x.detach()), fake)\n","    d_y_loss = (d_y_real_loss + d_y_fake_loss) * 0.5\n","\n","    d_loss = d_x_loss + d_y_loss\n","    d_loss.backward()\n","\n","    optimizer_d.step()\n","\n","    return g_loss, f_loss, d_x_loss, d_y_loss\n","\n","# 훈련 루프\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    for real_x, real_y in dataloader:\n","        real_x, real_y = real_x.cuda(), real_y.cuda()\n","\n","        g_loss, f_loss, d_x_loss, d_y_loss = train_step(real_x, real_y)\n","\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], g_loss={g_loss.item()}, f_loss={f_loss.item()}, d_x_loss={d_x_loss.item()}, d_y_loss={d_y_loss.item()}\")\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision.utils as vutils\n","\n","# 데이터셋 로딩 및 변환\n","class ImageDataset(Dataset):\n","    def __init__(self, photo_dir, sketch_dir, transform=None):\n","        self.photo_paths = sorted([os.path.join(photo_dir, x) for x in os.listdir(photo_dir)])\n","        self.sketch_paths = sorted([os.path.join(sketch_dir, x) for x in os.listdir(sketch_dir)])\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.photo_paths)\n","\n","    def __getitem__(self, idx):\n","        photo = Image.open(self.photo_paths[idx]).convert('RGB')\n","        sketch = Image.open(self.sketch_paths[idx]).convert('RGB')\n","\n","        if self.transform:\n","            photo = self.transform(photo)\n","            sketch = self.transform(sketch)\n","\n","        return photo, sketch\n","\n","# 이미지 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),\n","    transforms.RandomCrop(256),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# 데이터셋 준비\n","dataset = ImageDataset('/content/drive/MyDrive/GAN_tutorial-main/photos', '/content/drive/MyDrive/GAN_tutorial-main/sketches', transform)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Generator 정의 (UNet 스타일)\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 정의\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# 생성자와 판별자 모델 초기화\n","generator_g = Generator().cuda()  # 사진 -> 스케치\n","generator_f = Generator().cuda()  # 스케치 -> 사진\n","discriminator_x = Discriminator().cuda()  # 판별자 (스케치)\n","discriminator_y = Discriminator().cuda()  # 판별자 (사진)\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = nn.BCELoss()\n","lambda_cycle = 10.0  # Cycle consistency loss scale\n","optimizer_g = optim.Adam(list(generator_g.parameters()) + list(generator_f.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_d = optim.Adam(list(discriminator_x.parameters()) + list(discriminator_y.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 훈련 함수\n","def train_step(real_x, real_y):\n","    batch_size = real_x.size(0)\n","    valid = torch.ones(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","    fake = torch.zeros(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","\n","    # Generator 학습\n","    optimizer_g.zero_grad()\n","\n","    # 생성자 g의 손실 (스케치 -> 사진)\n","    fake_x = generator_g(real_y)\n","    g_loss = criterion(discriminator_y(fake_x), valid)  # 스케치 -> 사진 판별자\n","    g_loss.backward()\n","\n","    # 생성자 f의 손실 (사진 -> 스케치)\n","    fake_y = generator_f(real_x)\n","    f_loss = criterion(discriminator_x(fake_y), valid)  # 사진 -> 스케치 판별자\n","    f_loss.backward()\n","\n","    optimizer_g.step()\n","\n","    # Discriminator 학습\n","    optimizer_d.zero_grad()\n","\n","    # 판별자 x (스케치 판별)\n","    d_x_real_loss = criterion(discriminator_x(real_y), valid)\n","    d_x_fake_loss = criterion(discriminator_x(fake_y.detach()), fake)\n","    d_x_loss = (d_x_real_loss + d_x_fake_loss) * 0.5\n","\n","    # 판별자 y (사진 판별)\n","    d_y_real_loss = criterion(discriminator_y(real_x), valid)\n","    d_y_fake_loss = criterion(discriminator_y(fake_x.detach()), fake)\n","    d_y_loss = (d_y_real_loss + d_y_fake_loss) * 0.5\n","\n","    d_loss = d_x_loss + d_y_loss\n","    d_loss.backward()\n","\n","    optimizer_d.step()\n","\n","    return g_loss, f_loss, d_x_loss, d_y_loss, fake_x, fake_y\n","\n","# 이미지를 저장하는 함수\n","def save_generated_images(fake_x, fake_y, epoch, batch_idx):\n","    # fake_x와 fake_y를 저장합니다.\n","    vutils.save_image(fake_x.data, f'generated_image_photo_epoch_{epoch+1}_batch_{batch_idx+1}.jpg', normalize=True)\n","    vutils.save_image(fake_y.data, f'generated_image_sketch_epoch_{epoch+1}_batch_{batch_idx+1}.jpg', normalize=True)\n","\n","# 모델 저장 함수\n","def save_models(epoch):\n","    torch.save(generator_g.state_dict(), f'generator_g_epoch_{epoch+1}.pth')\n","    torch.save(generator_f.state_dict(), f'generator_f_epoch_{epoch+1}.pth')\n","    torch.save(discriminator_x.state_dict(), f'discriminator_x_epoch_{epoch+1}.pth')\n","    torch.save(discriminator_y.state_dict(), f'discriminator_y_epoch_{epoch+1}.pth')\n","\n","# 훈련 루프\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    for batch_idx, (real_x, real_y) in enumerate(dataloader):\n","        real_x, real_y = real_x.cuda(), real_y.cuda()\n","\n","        # 학습 단계\n","        g_loss, f_loss, d_x_loss, d_y_loss, fake_x, fake_y = train_step(real_x, real_y)\n","\n","        # 100번째 배치마다 이미지 저장\n","        if batch_idx % 100 == 0:\n","            save_generated_images(fake_x, fake_y, epoch, batch_idx)\n","\n","    # 에폭 끝날 때마다 모델 저장\n","    save_models(epoch)\n","\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], g_loss={g_loss.item()}, f_loss={f_loss.item()}, d_x_loss={d_x_loss.item()}, d_y_loss={d_y_loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJ2IQA4fyDI_","executionInfo":{"status":"ok","timestamp":1733905294149,"user_tz":-540,"elapsed":449393,"user":{"displayName":"현우","userId":"13721909872316796663"}},"outputId":"fe6be2f1-df14-4537-cfd3-8fcee9d8f8fd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], g_loss=0.8352916240692139, f_loss=0.7455977201461792, d_x_loss=0.6737101078033447, d_y_loss=0.710139274597168\n","Epoch [2/100], g_loss=0.8419185280799866, f_loss=0.7243838906288147, d_x_loss=0.6933859586715698, d_y_loss=0.755504846572876\n","Epoch [3/100], g_loss=0.805988073348999, f_loss=0.695401668548584, d_x_loss=0.6854727268218994, d_y_loss=0.583869993686676\n","Epoch [4/100], g_loss=0.7378864884376526, f_loss=0.6463990807533264, d_x_loss=0.704510509967804, d_y_loss=0.5676020383834839\n","Epoch [5/100], g_loss=0.8188287019729614, f_loss=0.7490993738174438, d_x_loss=0.6903992295265198, d_y_loss=0.6325308084487915\n","Epoch [6/100], g_loss=0.9038805961608887, f_loss=0.6844819784164429, d_x_loss=0.6879574060440063, d_y_loss=0.6273003816604614\n","Epoch [7/100], g_loss=0.7847558259963989, f_loss=0.7143215537071228, d_x_loss=0.6971917152404785, d_y_loss=0.6211123466491699\n","Epoch [8/100], g_loss=0.9140084981918335, f_loss=0.6988698840141296, d_x_loss=0.6907092332839966, d_y_loss=0.610450029373169\n","Epoch [9/100], g_loss=1.0011837482452393, f_loss=0.8214200139045715, d_x_loss=0.6917988061904907, d_y_loss=0.5678537487983704\n","Epoch [10/100], g_loss=0.9388154745101929, f_loss=0.8109966516494751, d_x_loss=0.6606042981147766, d_y_loss=0.592650294303894\n","Epoch [11/100], g_loss=0.9565753936767578, f_loss=0.7187691926956177, d_x_loss=0.6795639395713806, d_y_loss=0.554556131362915\n","Epoch [12/100], g_loss=1.037437915802002, f_loss=0.6964719295501709, d_x_loss=0.6958376169204712, d_y_loss=0.5866609811782837\n","Epoch [13/100], g_loss=1.0229618549346924, f_loss=0.7071120142936707, d_x_loss=0.692247748374939, d_y_loss=0.5606769323348999\n","Epoch [14/100], g_loss=1.012275218963623, f_loss=0.6719921231269836, d_x_loss=0.6707025766372681, d_y_loss=0.5971667766571045\n","Epoch [15/100], g_loss=0.763336718082428, f_loss=0.7209082841873169, d_x_loss=0.6858674883842468, d_y_loss=0.6613494157791138\n","Epoch [16/100], g_loss=0.9465590715408325, f_loss=0.6817030906677246, d_x_loss=0.6863261461257935, d_y_loss=0.589896559715271\n","Epoch [17/100], g_loss=1.0673387050628662, f_loss=0.6838657855987549, d_x_loss=0.7052643299102783, d_y_loss=0.6055182218551636\n","Epoch [18/100], g_loss=1.0869572162628174, f_loss=0.8874555826187134, d_x_loss=0.6545954942703247, d_y_loss=0.5965474843978882\n","Epoch [19/100], g_loss=1.1713923215866089, f_loss=0.6711896657943726, d_x_loss=0.6953499913215637, d_y_loss=0.5677189826965332\n","Epoch [20/100], g_loss=1.1621415615081787, f_loss=0.697445809841156, d_x_loss=0.6923356056213379, d_y_loss=0.6428052186965942\n","Epoch [21/100], g_loss=0.8841447830200195, f_loss=0.7082980871200562, d_x_loss=0.6969597339630127, d_y_loss=0.5440927743911743\n","Epoch [22/100], g_loss=1.103952169418335, f_loss=0.6993337869644165, d_x_loss=0.7092863321304321, d_y_loss=0.4656473398208618\n","Epoch [23/100], g_loss=1.293661117553711, f_loss=0.6775062680244446, d_x_loss=0.7010883092880249, d_y_loss=0.4369262456893921\n","Epoch [24/100], g_loss=1.0227382183074951, f_loss=0.6418520212173462, d_x_loss=0.6965134143829346, d_y_loss=0.47271230816841125\n","Epoch [25/100], g_loss=1.2200820446014404, f_loss=0.6971196532249451, d_x_loss=0.6844505071640015, d_y_loss=0.5235446691513062\n","Epoch [26/100], g_loss=1.179077386856079, f_loss=0.7100952863693237, d_x_loss=0.6688840389251709, d_y_loss=0.4206836521625519\n","Epoch [27/100], g_loss=1.0268505811691284, f_loss=0.650845468044281, d_x_loss=0.6759384870529175, d_y_loss=0.49924236536026\n","Epoch [28/100], g_loss=1.34177565574646, f_loss=0.6533519625663757, d_x_loss=0.6875041127204895, d_y_loss=0.5777396559715271\n","Epoch [29/100], g_loss=0.7771404981613159, f_loss=0.710210919380188, d_x_loss=0.684222936630249, d_y_loss=0.629885196685791\n","Epoch [30/100], g_loss=1.1538618803024292, f_loss=0.7357250452041626, d_x_loss=0.6807639598846436, d_y_loss=0.4678241014480591\n","Epoch [31/100], g_loss=1.2157564163208008, f_loss=0.8405868411064148, d_x_loss=0.6442850232124329, d_y_loss=0.5100122094154358\n","Epoch [32/100], g_loss=1.5418620109558105, f_loss=0.6901174187660217, d_x_loss=0.6951338648796082, d_y_loss=0.3607754111289978\n","Epoch [33/100], g_loss=1.1059107780456543, f_loss=0.7945072650909424, d_x_loss=0.673991322517395, d_y_loss=0.47682061791419983\n","Epoch [34/100], g_loss=1.2293152809143066, f_loss=0.6837760210037231, d_x_loss=0.695528507232666, d_y_loss=0.5502691268920898\n","Epoch [35/100], g_loss=0.864202618598938, f_loss=0.7080926895141602, d_x_loss=0.6932973861694336, d_y_loss=0.5686050653457642\n","Epoch [36/100], g_loss=1.4318501949310303, f_loss=0.7220473289489746, d_x_loss=0.6899751424789429, d_y_loss=0.5248645544052124\n","Epoch [37/100], g_loss=1.4126163721084595, f_loss=0.6820336580276489, d_x_loss=0.690985918045044, d_y_loss=0.43130671977996826\n","Epoch [38/100], g_loss=1.3465015888214111, f_loss=0.6974594593048096, d_x_loss=0.6824461221694946, d_y_loss=0.6220079064369202\n","Epoch [39/100], g_loss=1.316147804260254, f_loss=0.6520681977272034, d_x_loss=0.694415807723999, d_y_loss=0.4628036618232727\n","Epoch [40/100], g_loss=1.3796095848083496, f_loss=0.6640100479125977, d_x_loss=0.7052977681159973, d_y_loss=0.5680078268051147\n","Epoch [41/100], g_loss=1.4213684797286987, f_loss=0.7188723683357239, d_x_loss=0.656683087348938, d_y_loss=0.56591796875\n","Epoch [42/100], g_loss=1.506251335144043, f_loss=0.6954596042633057, d_x_loss=0.6787300109863281, d_y_loss=0.39329439401626587\n","Epoch [43/100], g_loss=1.1468170881271362, f_loss=0.6814630627632141, d_x_loss=0.6886571645736694, d_y_loss=0.4783857464790344\n","Epoch [44/100], g_loss=1.4824799299240112, f_loss=0.7163053750991821, d_x_loss=0.6716864109039307, d_y_loss=0.4820525348186493\n","Epoch [45/100], g_loss=1.037540316581726, f_loss=0.6697444319725037, d_x_loss=0.6850869059562683, d_y_loss=0.568207859992981\n","Epoch [46/100], g_loss=1.65812349319458, f_loss=0.7218915224075317, d_x_loss=0.7016962766647339, d_y_loss=0.30141615867614746\n","Epoch [47/100], g_loss=1.4261174201965332, f_loss=0.8229720592498779, d_x_loss=0.6909720301628113, d_y_loss=0.2973422110080719\n","Epoch [48/100], g_loss=1.2012052536010742, f_loss=0.6606289148330688, d_x_loss=0.6860619783401489, d_y_loss=0.5299779176712036\n","Epoch [49/100], g_loss=1.1974945068359375, f_loss=0.6796025633811951, d_x_loss=0.6905081272125244, d_y_loss=0.5336823463439941\n","Epoch [50/100], g_loss=1.4553790092468262, f_loss=0.713698148727417, d_x_loss=0.6889582872390747, d_y_loss=0.41160136461257935\n","Epoch [51/100], g_loss=1.3188717365264893, f_loss=0.6781096458435059, d_x_loss=0.6731042861938477, d_y_loss=0.5646275877952576\n","Epoch [52/100], g_loss=1.260543942451477, f_loss=0.6942490339279175, d_x_loss=0.6905841827392578, d_y_loss=0.5125753283500671\n","Epoch [53/100], g_loss=1.7545342445373535, f_loss=0.7101312279701233, d_x_loss=0.7155188322067261, d_y_loss=0.3953649401664734\n","Epoch [54/100], g_loss=1.26543390750885, f_loss=0.6824689507484436, d_x_loss=0.6856468319892883, d_y_loss=0.664652943611145\n","Epoch [55/100], g_loss=1.0243250131607056, f_loss=0.6977670192718506, d_x_loss=0.6949092149734497, d_y_loss=0.5513570308685303\n","Epoch [56/100], g_loss=1.1882551908493042, f_loss=0.7129158973693848, d_x_loss=0.6775694489479065, d_y_loss=0.5987003445625305\n","Epoch [57/100], g_loss=0.9345608949661255, f_loss=0.7057064175605774, d_x_loss=0.6740608215332031, d_y_loss=0.6061463356018066\n","Epoch [58/100], g_loss=0.9812252521514893, f_loss=0.7967683672904968, d_x_loss=0.6885381937026978, d_y_loss=0.56783127784729\n","Epoch [59/100], g_loss=0.8157802820205688, f_loss=0.6778163909912109, d_x_loss=0.6880857944488525, d_y_loss=0.695586085319519\n","Epoch [60/100], g_loss=1.0215494632720947, f_loss=0.6585971117019653, d_x_loss=0.6817699074745178, d_y_loss=0.6296918392181396\n","Epoch [61/100], g_loss=0.9769971966743469, f_loss=0.7019034028053284, d_x_loss=0.6876859068870544, d_y_loss=0.5727988481521606\n","Epoch [62/100], g_loss=1.0123729705810547, f_loss=0.6899582147598267, d_x_loss=0.6951935887336731, d_y_loss=0.5718978047370911\n","Epoch [63/100], g_loss=0.9378606081008911, f_loss=0.7095550298690796, d_x_loss=0.7024557590484619, d_y_loss=0.6189643144607544\n","Epoch [64/100], g_loss=1.2758177518844604, f_loss=0.7017568945884705, d_x_loss=0.6883525848388672, d_y_loss=0.46138137578964233\n","Epoch [65/100], g_loss=0.9904232621192932, f_loss=0.7686667442321777, d_x_loss=0.6622022986412048, d_y_loss=0.5946400165557861\n","Epoch [66/100], g_loss=1.0336833000183105, f_loss=0.713056206703186, d_x_loss=0.691437840461731, d_y_loss=0.5249881744384766\n","Epoch [67/100], g_loss=0.9517370462417603, f_loss=0.700610339641571, d_x_loss=0.6892960667610168, d_y_loss=0.6592510342597961\n","Epoch [68/100], g_loss=0.8879197239875793, f_loss=0.6970325708389282, d_x_loss=0.6949729919433594, d_y_loss=0.48779556155204773\n","Epoch [69/100], g_loss=1.0945169925689697, f_loss=0.6454450488090515, d_x_loss=0.6882965564727783, d_y_loss=0.5508383512496948\n","Epoch [70/100], g_loss=1.085585355758667, f_loss=0.6768181324005127, d_x_loss=0.6923887729644775, d_y_loss=0.6322134733200073\n","Epoch [71/100], g_loss=1.0012104511260986, f_loss=0.6800718307495117, d_x_loss=0.6815615892410278, d_y_loss=0.510114312171936\n","Epoch [72/100], g_loss=1.2512810230255127, f_loss=0.6911224126815796, d_x_loss=0.6923354268074036, d_y_loss=0.4907901883125305\n","Epoch [73/100], g_loss=1.1351768970489502, f_loss=0.746904730796814, d_x_loss=0.7172838449478149, d_y_loss=0.5612054467201233\n","Epoch [74/100], g_loss=1.1531412601470947, f_loss=0.6770583987236023, d_x_loss=0.687969446182251, d_y_loss=0.4980401396751404\n","Epoch [75/100], g_loss=1.2148523330688477, f_loss=0.7393020391464233, d_x_loss=0.6832256317138672, d_y_loss=0.5342156291007996\n","Epoch [76/100], g_loss=1.239644169807434, f_loss=0.70955491065979, d_x_loss=0.6893331408500671, d_y_loss=0.44545865058898926\n","Epoch [77/100], g_loss=1.2800912857055664, f_loss=0.697752833366394, d_x_loss=0.6976272463798523, d_y_loss=0.4920155704021454\n","Epoch [78/100], g_loss=1.371652603149414, f_loss=0.7100998163223267, d_x_loss=0.6994428038597107, d_y_loss=0.41521745920181274\n","Epoch [79/100], g_loss=1.140413522720337, f_loss=0.7226262092590332, d_x_loss=0.7040153741836548, d_y_loss=0.4393109381198883\n","Epoch [80/100], g_loss=1.1520278453826904, f_loss=0.8065588474273682, d_x_loss=0.6807700395584106, d_y_loss=0.43525660037994385\n","Epoch [81/100], g_loss=1.2321875095367432, f_loss=0.6671297550201416, d_x_loss=0.6861765384674072, d_y_loss=0.6006242036819458\n","Epoch [82/100], g_loss=1.5711016654968262, f_loss=0.6923961639404297, d_x_loss=0.6795803308486938, d_y_loss=0.3426899313926697\n","Epoch [83/100], g_loss=1.3384785652160645, f_loss=0.6785104870796204, d_x_loss=0.6832675933837891, d_y_loss=0.6173240542411804\n","Epoch [84/100], g_loss=1.1789910793304443, f_loss=0.6922696232795715, d_x_loss=0.6827014088630676, d_y_loss=0.5825967192649841\n","Epoch [85/100], g_loss=1.0458637475967407, f_loss=0.7289960980415344, d_x_loss=0.6892776489257812, d_y_loss=0.5457686185836792\n","Epoch [86/100], g_loss=0.9769620895385742, f_loss=0.747165858745575, d_x_loss=0.6851104497909546, d_y_loss=0.4808049499988556\n","Epoch [87/100], g_loss=1.1036725044250488, f_loss=0.6925128698348999, d_x_loss=0.7079242467880249, d_y_loss=0.510339617729187\n","Epoch [88/100], g_loss=1.4355299472808838, f_loss=0.6914433240890503, d_x_loss=0.682212233543396, d_y_loss=0.43667811155319214\n","Epoch [89/100], g_loss=1.4066827297210693, f_loss=0.7624466419219971, d_x_loss=0.6756182909011841, d_y_loss=0.5389118194580078\n","Epoch [90/100], g_loss=1.2877771854400635, f_loss=0.6788196563720703, d_x_loss=0.7010405659675598, d_y_loss=0.43825265765190125\n","Epoch [91/100], g_loss=1.211616039276123, f_loss=0.6928397417068481, d_x_loss=0.7010793685913086, d_y_loss=0.5287624597549438\n","Epoch [92/100], g_loss=1.2916696071624756, f_loss=0.7166619300842285, d_x_loss=0.6857877969741821, d_y_loss=0.4577694535255432\n","Epoch [93/100], g_loss=1.1646089553833008, f_loss=0.729248046875, d_x_loss=0.7002438902854919, d_y_loss=0.46663057804107666\n","Epoch [94/100], g_loss=1.3399081230163574, f_loss=0.7178766131401062, d_x_loss=0.6961207389831543, d_y_loss=0.47360920906066895\n","Epoch [95/100], g_loss=1.0406358242034912, f_loss=0.8071670532226562, d_x_loss=0.663469672203064, d_y_loss=0.62852942943573\n","Epoch [96/100], g_loss=1.0918066501617432, f_loss=0.7661926746368408, d_x_loss=0.6496750116348267, d_y_loss=0.6627333164215088\n","Epoch [97/100], g_loss=1.328723430633545, f_loss=0.6791242361068726, d_x_loss=0.6845231056213379, d_y_loss=0.5213008522987366\n","Epoch [98/100], g_loss=1.2190736532211304, f_loss=0.7181602120399475, d_x_loss=0.6919978260993958, d_y_loss=0.48755329847335815\n","Epoch [99/100], g_loss=1.2523261308670044, f_loss=0.7388256788253784, d_x_loss=0.6895323991775513, d_y_loss=0.47516748309135437\n","Epoch [100/100], g_loss=1.3192148208618164, f_loss=0.8078615665435791, d_x_loss=0.6773485541343689, d_y_loss=0.5614701509475708\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import os\n","\n","# 모델 로딩 함수\n","def load_model(model, path):\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","\n","# 이미지 변환 함수\n","def transform_image(image_path, transform):\n","    image = Image.open(image_path).convert('RGB')\n","    image = transform(image).unsqueeze(0).cuda()  # 배치 차원을 추가하고 GPU로 이동\n","    return image\n","\n","# 이미지 저장 함수\n","def save_image(tensor, save_path):\n","    # 이미지 출력을 0~1 범위로 되돌려 놓고 저장\n","    unnormalized_image = tensor.cpu().detach().squeeze(0)\n","    unnormalized_image = unnormalized_image * 0.5 + 0.5  # [-1, 1] to [0, 1]\n","    save_image = transforms.ToPILImage()(unnormalized_image)\n","    save_image.save(save_path)\n","\n","# 이미지 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),\n","    transforms.CenterCrop(256),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# 모델 로드\n","generator_f_path = 'generator_f.pth'  # 훈련 후 저장된 모델 파일 경로\n","generator_f = Generator().cuda()  # 사진 -> 스케치 생성기 (가짜 이미지 생성)\n","load_model(generator_f, generator_f_path)\n","\n","# 이미지 변환\n","sketch_image_path = 'image.jpg'  # 변환할 스케치 이미지 파일 경로\n","transformed_image = transform_image(sketch_image_path, transform)\n","\n","# 생성된 이미지를 사진으로 변환\n","with torch.no_grad():\n","    fake_photo = generator_f(transformed_image)\n","\n","# 변환된 이미지 저장\n","save_image(fake_photo, 'generated_image.jpg')\n","print(\"사진으로 변환된 이미지를 'generated_image.jpg'로 저장했습니다.\")\n"],"metadata":{"id":"vdGu_Z080QXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 모델 로딩 함수\n","def load_model(model, path):\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","\n","# 이미지 변환 함수\n","def transform_image(image_path, transform):\n","    image = Image.open(image_path).convert('RGB')  # 이미지를 RGB로 변환\n","    image = transform(image).unsqueeze(0).cuda()  # 텐서로 변환하고 배치 차원 추가, GPU로 이동\n","    return image\n","\n","# 이미지 저장 함수\n","def save_image(tensor, save_path):\n","    # [-1, 1] 범위의 텐서를 [0, 1] 범위로 변환하고 이미지를 저장\n","    unnormalized_image = tensor.cpu().detach().squeeze(0)\n","    unnormalized_image = unnormalized_image * 0.5 + 0.5  # [-1, 1] -> [0, 1]\n","    save_image = transforms.ToPILImage()(unnormalized_image)\n","    save_image.save(save_path)\n","\n","# 모델 및 전처리 설정\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),  # 모델 입력 크기에 맞게 크기 조정\n","    transforms.CenterCrop(256),  # 중앙 크롭\n","    transforms.ToTensor(),  # 텐서로 변환\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n","])\n","\n","# 모델 로드 (코랩에 있는 모델 불러오기)\n","generator_g = Generator().cuda()  # 생성자 모델 초기화\n","generator_g_path = '/content/generator_g_epoch_99.pth'  # 코랩에서 모델 경로\n","load_model(generator_g, generator_g_path)\n","\n","# 이미지 경로 설정 (구글 드라이브에 있는 이미지)\n","sketch_image_path = '/content/drive/MyDrive/GAN_tutorial-main/image.jpg'  # 구글 드라이브에서 이미지 경로\n","transformed_image = transform_image(sketch_image_path, transform)\n","\n","# 생성된 이미지를 사진으로 변환\n","with torch.no_grad():\n","    fake_photo = generator_g(transformed_image)  # 스케치를 사진으로 변환\n","\n","# 변환된 이미지 저장 (구글 드라이브에 저장)\n","save_image(fake_photo, '/content/drive/MyDrive/GAN_tutorial-main/generated_image.jpg')  # 구글 드라이브에 저장\n","print(\"스케치 이미지가 사진으로 변환되어 'generated_image.jpg'로 저장되었습니다.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9s3k_FDj0Qzs","executionInfo":{"status":"ok","timestamp":1733905705901,"user_tz":-540,"elapsed":2634,"user":{"displayName":"현우","userId":"13721909872316796663"}},"outputId":"6260056c-b7db-4a65-f23d-c758088b4ff3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","스케치 이미지가 사진으로 변환되어 'generated_image.jpg'로 저장되었습니다.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-0681ed797f1d>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(path))\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 모델 로딩 함수\n","def load_model(model, path):\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","\n","# 이미지 변환 함수\n","def transform_image(image_path, transform):\n","    image = Image.open(image_path).convert('RGB')  # 이미지를 RGB로 변환\n","    image = transform(image).unsqueeze(0).cuda()  # 텐서로 변환하고 배치 차원 추가, GPU로 이동\n","    return image\n","\n","# 이미지 저장 함수\n","def save_image(tensor, save_path):\n","    # [-1, 1] 범위의 텐서를 [0, 1] 범위로 변환하고 이미지를 저장\n","    unnormalized_image = tensor.cpu().detach().squeeze(0)\n","    unnormalized_image = (unnormalized_image + 1) / 2  # [-1, 1] -> [0, 1] 범위로 변환\n","    save_image = transforms.ToPILImage()(unnormalized_image)\n","    save_image.save(save_path)\n","\n","# 모델 및 전처리 설정\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),  # 모델 입력 크기에 맞게 크기 조정\n","    transforms.CenterCrop(256),  # 중앙 크롭\n","    transforms.ToTensor(),  # 텐서로 변환\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 정규화\n","])\n","\n","# 모델 로드 (코랩에 있는 모델 불러오기)\n","generator_g = Generator().cuda()  # 생성자 모델 초기화\n","generator_g_path = '/content/generator_g_epoch_80.pth'  # 코랩에서 모델 경로\n","load_model(generator_g, generator_g_path)\n","\n","# 이미지 경로 설정 (구글 드라이브에 있는 이미지)\n","sketch_image_path = '/content/drive/MyDrive/GAN_tutorial-main/image.jpg'  # 구글 드라이브에서 이미지 경로\n","transformed_image = transform_image(sketch_image_path, transform)\n","\n","# 생성된 이미지를 사진으로 변환\n","with torch.no_grad():\n","    fake_photo = generator_g(transformed_image)  # 스케치를 사진으로 변환\n","\n","# 변환된 이미지 저장 (구글 드라이브에 저장)\n","save_image(fake_photo, '/content/drive/MyDrive/GAN_tutorial-main/generated_image2324.jpg')  # 구글 드라이브에 저장\n","print(\"스케치 이미지가 사진으로 변환되어 'generated_image.jpg'로 저장되었습니다.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4zgq02I1nj1","executionInfo":{"status":"ok","timestamp":1733905886919,"user_tz":-540,"elapsed":2734,"user":{"displayName":"현우","userId":"13721909872316796663"}},"outputId":"6bebd530-bc4f-4418-ee63-5bef6e054891"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","스케치 이미지가 사진으로 변환되어 'generated_image.jpg'로 저장되었습니다.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-12-8c9ffd8b7513>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(path))\n"]}]},{"cell_type":"markdown","source":["알겠습니다! 이제 모델을 저장하고, 생성된 이미지를 메모리에서 확인할 수 있도록 저장하는 방법을 설명하겠습니다. 아래에 **모델 저장**과 **생성된 이미지 저장**을 포함한 전체적인 예제를 제공합니다. 이 예제는 훈련 중 생성된 이미지를 `image.jpg`로 저장하고, 훈련이 완료되면 모델을 저장하는 기능을 추가한 코드입니다.\n","\n","### 코드 수정 내용:\n","1. **이미지 저장**: 생성된 이미지를 `image.jpg`와 같은 형식으로 저장합니다.\n","2. **모델 저장**: 훈련 후 `generator`와 `discriminator` 모델을 `.pth` 파일로 저장합니다.\n","\n","### 전체 코드:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision.utils as vutils\n","\n","# 데이터셋 로딩 및 변환\n","class ImageDataset(Dataset):\n","    def __init__(self, photo_dir, sketch_dir, transform=None):\n","        self.photo_paths = sorted([os.path.join(photo_dir, x) for x in os.listdir(photo_dir)])\n","        self.sketch_paths = sorted([os.path.join(sketch_dir, x) for x in os.listdir(sketch_dir)])\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.photo_paths)\n","\n","    def __getitem__(self, idx):\n","        photo = Image.open(self.photo_paths[idx]).convert('RGB')\n","        sketch = Image.open(self.sketch_paths[idx]).convert('RGB')\n","\n","        if self.transform:\n","            photo = self.transform(photo)\n","            sketch = self.transform(sketch)\n","\n","        return photo, sketch\n","\n","# 이미지 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((286, 286)),\n","    transforms.RandomCrop(256),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# 데이터셋 준비\n","dataset = ImageDataset('/content/drive/MyDrive/GAN_tutorial-main/photos', '/content/drive/MyDrive/GAN_tutorial-main/sketches', transform)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Generator 정의 (UNet 스타일)\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 정의\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# 생성자와 판별자 모델 초기화\n","generator_g = Generator().cuda()  # 사진 -> 스케치\n","generator_f = Generator().cuda()  # 스케치 -> 사진\n","discriminator_x = Discriminator().cuda()  # 판별자 (스케치)\n","discriminator_y = Discriminator().cuda()  # 판별자 (사진)\n","\n","# 손실 함수 및 옵티마이저 설정\n","criterion = nn.BCELoss()\n","lambda_cycle = 10.0  # Cycle consistency loss scale\n","optimizer_g = optim.Adam(list(generator_g.parameters()) + list(generator_f.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_d = optim.Adam(list(discriminator_x.parameters()) + list(discriminator_y.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 훈련 함수\n","def train_step(real_x, real_y):\n","    batch_size = real_x.size(0)\n","    valid = torch.ones(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","    fake = torch.zeros(batch_size, 1, 32, 32, device=real_x.device)  # Adjust shape to match discriminator output\n","\n","    # Generator 학습\n","    optimizer_g.zero_grad()\n","\n","    # 생성자 g의 손실 (스케치 -> 사진)\n","    fake_x = generator_g(real_y)\n","    g_loss = criterion(discriminator_y(fake_x), valid)  # 스케치 -> 사진 판별자\n","    g_loss.backward()\n","\n","    # 생성자 f의 손실 (사진 -> 스케치)\n","    fake_y = generator_f(real_x)\n","    f_loss = criterion(discriminator_x(fake_y), valid)  # 사진 -> 스케치 판별자\n","    f_loss.backward()\n","\n","    optimizer_g.step()\n","\n","    # Discriminator 학습\n","    optimizer_d.zero_grad()\n","\n","    # 판별자 x (스케치 판별)\n","    d_x_real_loss = criterion(discriminator_x(real_y), valid)\n","    d_x_fake_loss = criterion(discriminator_x(fake_y.detach()), fake)\n","    d_x_loss = (d_x_real_loss + d_x_fake_loss) * 0.5\n","\n","    # 판별자 y (사진 판별)\n","    d_y_real_loss = criterion(discriminator_y(real_x), valid)\n","    d_y_fake_loss = criterion(discriminator_y(fake_x.detach()), fake)\n","    d_y_loss = (d_y_real_loss + d_y_fake_loss) * 0.5\n","\n","    d_loss = d_x_loss + d_y_loss\n","    d_loss.backward()\n","\n","    optimizer_d.step()\n","\n","    return g_loss, f_loss, d_x_loss, d_y_loss, fake_x, fake_y\n","\n","# 이미지를 저장하는 함수\n","def save_generated_images(fake_x, fake_y, epoch, batch_idx):\n","    # fake_x와 fake_y를 저장합니다.\n","    vutils.save_image(fake_x.data, f'generated_image_photo_epoch_{epoch+1}_batch_{batch_idx+1}.jpg', normalize=True)\n","    vutils.save_image(fake_y.data, f'generated_image_sketch_epoch_{epoch+1}_batch_{batch_idx+1}.jpg', normalize=True)\n","\n","# 모델 저장 함수\n","def save_models(epoch):\n","    torch.save(generator_g.state_dict(), f'generator_g_epoch_{epoch+1}.pth')\n","    torch.save(generator_f.state_dict(), f'generator_f_epoch_{epoch+1}.pth')\n","    torch.save(discriminator_x.state_dict(), f'discriminator_x_epoch_{epoch+1}.pth')\n","    torch.save(discriminator_y.state_dict(), f'discriminator_y_epoch_{epoch+1}.pth')\n","\n","# 훈련 루프\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    for batch_idx, (real_x, real_y) in enumerate(dataloader):\n","        real_x, real_y = real_x.cuda(), real_y.cuda()\n","\n","        # 학습 단계\n","        g_loss, f_loss, d_x_loss, d_y_loss, fake_x, fake_y = train_step(real_x, real_y)\n","\n","        # 100번째 배치마다 이미지 저장\n","        if batch_idx % 100 == 0:\n","            save_generated_images(fake_x, fake_y, epoch, batch_idx)\n","\n","    # 에폭 끝날 때마다 모델 저장\n","    save_models(epoch)\n","\n","    print(f\"Epoch [{epoch+1}/{EPOCHS}], g_loss={g_loss.item()}, f_loss={f_loss.item()}, d_x_loss={d_x_loss.item()}, d_y_loss={d_y_loss.item()}\")\n","```\n","\n","### 주요 수정 사항:\n","1. **이미지 저장**: `save_generated_images` 함수는 각 배치마다 생성된 `fake_x` (스케치에서 사진으로 변환된 이미지)와 `fake_y` (사진에서 스케치로 변환된 이미지)를 `.jpg` 형식으로 저장합니다.\n","   - 이미지 파일 이름은 `generated_image_photo_epoch_{epoch+1}_batch_{batch_idx+1}.jpg`와 `generated_image_sketch_epoch_{epoch+1}_batch_{batch_idx+1}.jpg`와 같은 형식으로 저장됩니다.\n","   \n","2. **모델 저장**: 훈련이 끝날 때마다 `save_models` 함수가 호출되어 각 epoch마다 모델을 `.pth` 파일로 저장합니다.\n","   - 파일 이름은 `generator_g_epoch_{epoch+1}.pth`, `discriminator_x_epoch_{epoch+1}.pth` 형식으로 저장됩니다.\n","\n","### 추가적으로:\n","- **모델 저장 후 재사용**\n","\n",": 모델을 저장한 후, 다른 코드에서 `torch.load()`로 로드하여 사용할 수 있습니다.\n","- **이미지 저장**: 생성된 이미지를 지정한 경로에 저장하여 모델의 출력을 확인할 수 있습니다."],"metadata":{"id":"RKpFQa7-u3V_"}},{"cell_type":"code","source":[],"metadata":{"id":"TQ_ZzggVtkYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p4J56EL7tl68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DwTVQsRAtmzZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이해했습니다! `GAN_tutorial-main` 폴더 안에서 작업을 진행한다고 하셨으므로, 해당 경로와 설정을 맞춰서 CycleGAN을 구현하는 코드로 다시 작성하겠습니다.\n","\n","### 1. 라이브러리 설치 및 임포트\n","\n","먼저, 필요한 라이브러리를 설치하고 임포트합니다. 이 코드는 **CycleGAN** 모델을 구성하는 데 필요한 라이브러리를 포함하고 있습니다.\n","\n","```python\n","!pip install -q git+https://github.com/tensorflow/examples.git\n","import tensorflow as tf\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from tensorflow_examples.models.pix2pix import pix2pix\n","```\n","\n","### 2. 구글 드라이브 마운트\n","\n","구글 드라이브에 있는 `photos`와 `sketches` 폴더를 사용하기 위해, 먼저 드라이브를 마운트합니다.\n","\n","```python\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","\n","### 3. 데이터셋 경로 설정\n","\n","구글 드라이브에서 `GAN_tutorial-main` 폴더 내에 `photos`와 `sketches` 폴더가 존재하는 경로를 설정합니다. 해당 경로는 **자신의 드라이브에 맞게 수정**해야 합니다.\n","\n","```python\n","# 구글 드라이브에서 데이터셋 경로 설정\n","base_dir = '/content/drive/MyDrive/GAN_tutorial-main/'  # 'GAN_tutorial-main' 폴더 경로\n","photos_path = os.path.join(base_dir, 'photos')\n","sketches_path = os.path.join(base_dir, 'sketches')\n","\n","# 이미지 크기 설정\n","IMG_HEIGHT = 256\n","IMG_WIDTH = 256\n","BATCH_SIZE = 1\n","```\n","\n","### 4. 이미지 전처리 함수\n","\n","이미지를 불러오고 전처리하는 함수들입니다. 여기서는 이미지를 `256x256` 크기로 자르고 정규화하여 CycleGAN 모델에 적합하게 만듭니다.\n","\n","```python\n","# 이미지 무작위 자르기 함수\n","def random_crop(image):\n","    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, 3])  # 256x256 크기로 자릅니다.\n","    return cropped_image\n","\n","# 이미지 정규화 함수\n","def normalize(image):\n","    image = tf.cast(image, tf.float32)  # 이미지 타입을 float32로 변환\n","    image = (image / 127.5) - 1  # [-1, 1] 범위로 정규화\n","    return image\n","\n","# 이미지 무작위 지터링 함수\n","def random_jitter(image):\n","    # 이미지 크기를 286x286으로 리사이즈합니다. 더 큰 크기로 변경한 후 자르기 위해 사용합니다.\n","    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    # 256x256 크기로 무작위 자르기 진행\n","    image = random_crop(image)\n","    # 50% 확률로 이미지를 좌우 반전하여 데이터 다양성 증가\n","    image = tf.image.random_flip_left_right(image)\n","    return image\n","```\n","\n","### 5. 데이터셋 불러오기\n","\n","`photos`와 `sketches` 폴더에서 이미지를 쌍으로 불러오는 함수입니다.\n","\n","```python\n","# 사진과 스케치 이미지 불러오기 함수\n","def load_image(image_path):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = random_jitter(image)  # 이미지에 무작위 지터링 적용\n","    image = normalize(image)\n","    return image\n","\n","# 파일 경로 로드\n","def load_dataset(photo_dir, sketch_dir):\n","    photos = sorted([os.path.join(photo_dir, x) for x in os.listdir(photo_dir)])\n","    sketches = sorted([os.path.join(sketch_dir, x) for x in os.listdir(sketch_dir)])\n","    return photos, sketches\n","\n","# 데이터셋 로드\n","photos_images, sketches_images = load_dataset(photos_path, sketches_path)\n","```\n","\n","### 6. 모델 정의\n","\n","CycleGAN에서 사용할 생성자(generator)와 판별자(discriminator)를 정의합니다. `pix2pix.unet_generator`와 `pix2pix.discriminator`를 사용하여 생성자와 판별자를 구성합니다.\n","\n","```python\n","# 생성자 모델 정의\n","OUTPUT_CHANNELS = 3  # RGB 이미지를 생성하기 위해 출력 채널 수를 3으로 설정합니다.\n","generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')  # 스케치 -> 사진\n","generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')  # 사진 -> 스케치\n","\n","# 판별자 모델 정의\n","discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)  # 스케치 이미지 판별\n","discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)  # 사진 이미지 판별\n","```\n","\n","### 7. 손실 함수 정의\n","\n","CycleGAN에서 사용하는 **생성자 손실**과 **판별자 손실** 함수입니다.\n","\n","```python\n","# 생성자 손실 함수\n","def generator_loss(disc_generated_output, gen_output, target, cycle_output, lambda_cycle=10.0):\n","    # GAN Loss\n","    gan_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_generated_output), logits=disc_generated_output))\n","    # Cycle Consistency Loss\n","    cycle_loss = tf.reduce_mean(tf.abs(target - cycle_output))\n","    # L1 loss 추가\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","    return gan_loss + lambda_cycle * cycle_loss + 100 * l1_loss\n","\n","# 판별자 손실 함수\n","def discriminator_loss(disc_real_output, disc_generated_output):\n","    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_real_output), logits=disc_real_output))\n","    generated_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(disc_generated_output), logits=disc_generated_output))\n","    return real_loss + generated_loss\n","```\n","\n","### 8. 최적화 및 훈련 단계 설정\n","\n","CycleGAN의 훈련을 위한 **최적화 함수**와 **훈련 단계**를 설정합니다.\n","\n","```python\n","# 최적화 함수 설정\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, decay_steps=100, decay_rate=0.96, staircase=True)\n","generator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n","```\n","\n","### 9. 훈련 루프\n","\n","훈련 루프에서는 생성자와 판별자의 손실을 계산하고, **실제 이미지와 생성된 이미지**를 비교하여 모델을 업데이트합니다. 아래는 훈련 루프의 예시입니다.\n","\n","```python\n","# 훈련 루프\n","@tf.function\n","def train_step(real_x, real_y):\n","    with tf.GradientTape(persistent=True) as tape:\n","        # 생성자 모델 예측\n","        fake_x = generator_g(real_y, training=True)\n","        fake_y = generator_f(real_x, training=True)\n","\n","        # 판별자 출력\n","        disc_real_x = discriminator_x(real_x, training=True)\n","        disc_fake_x = discriminator_x(fake_x, training=True)\n","\n","        disc_real_y = discriminator_y(real_y, training=True)\n","        disc_fake_y = discriminator_y(fake_y, training=True)\n","\n","        # 생성자 손실 계산\n","        g_loss = generator_loss(disc_fake_y, fake_y, real_y, fake_x)\n","        f_loss = generator_loss(disc_fake_x, fake_x, real_x, fake_y)\n","\n","        # 판별자 손실 계산\n","        d_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n","        d_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n","\n","    # 그라디언트 계산 및 최적화\n","    generator_g_gradients = tape.gradient(g_loss, generator_g.trainable_variables)\n","    generator_f_gradients = tape.gradient(f_loss, generator_f.trainable_variables)\n","    discriminator_x_gradients = tape.gradient(d_x_loss, discriminator_x.trainable_variables)\n","    discriminator_y_gradients = tape.gradient(d_y_loss, discriminator_y.trainable_variables)\n","\n","    # 최적화\n","    generator_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n","    generator_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n","\n","    return g_loss, f_loss, d_x_loss, d_y_loss\n","```\n","\n","### 10. 훈련 시작\n","\n","이제 훈련을 시작할 준비가 되었습니다. 여러 에폭을 반복하여 모델을 훈련시킬 수 있습니다.\n","\n","```python\n","# 에폭 반복\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    for image_x, image_y in zip(photos_images, sketches_images):\n","        real_x = load_image(image_x)\n","        real_y = load_image(image_y)\n","\n","        g_loss, f_loss, d_x_loss, d_y_loss = train_step(real_x\n","\n",", real_y)\n","\n","    # 훈련 진행 상황 출력\n","    print(f\"Epoch {epoch+1}/{EPOCHS}, g_loss={g_loss}, f_loss={f_loss}, d_x_loss={d_x_loss}, d_y_loss={d_y_loss}\")\n","```\n","\n","이 코드를 사용하여 `GAN_tutorial-main` 폴더에서 CycleGAN 모델을 훈련시키고, 구글 드라이브의 `photos`와 `sketches` 폴더에서 이미지를 불러와 훈련할 수 있습니다."],"metadata":{"id":"eXvyajulss57"}}]}